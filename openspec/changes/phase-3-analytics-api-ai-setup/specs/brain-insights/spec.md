## ADDED Requirements

### Requirement: brain-insights-endpoint
The system SHALL expose a POST `/api/brain/insights` endpoint that generates AI-powered natural-language insights.

#### Scenario: generate insights
- **WHEN** a POST request is made to `/api/brain/insights` with an optional market/motion/stage filter
- **THEN** the system queries recent funnel metrics, scores, and rep KPIs, passes them to the LLM with the active strategy mode injected into the system prompt, and returns categorized insights

#### Scenario: strategy mode shapes output
- **WHEN** the active strategy mode is "maximize_efficiency"
- **THEN** the AI prioritizes insights about CAC, payback period, and spend efficiency over growth metrics

### Requirement: insight-categorization
Each insight returned by the Brain SHALL be categorized by urgency level.

#### Scenario: strategic insight
- **WHEN** the AI identifies a trend-level change (e.g., channel velocity slowing)
- **THEN** the insight is categorized as "strategic"

#### Scenario: tactical insight
- **WHEN** the AI identifies an actionable issue (e.g., a campaign exceeding CAC target)
- **THEN** the insight is categorized as "tactical"

### Requirement: strategy-prompt-injection
The system prompt sent to the LLM SHALL include the active strategy mode, business rule guardrails, and a description of the Bowtie Funnel context.

#### Scenario: prompt contains guardrails
- **WHEN** the Brain endpoint constructs the LLM prompt
- **THEN** the system prompt includes max_cac_payback_months, max_churn_rate, arpa_min, arpa_max from the active strategy_config

### Requirement: streaming-response
The Brain insights endpoint SHALL support streaming responses via the Vercel AI SDK.

#### Scenario: streamed output
- **WHEN** the client requests insights
- **THEN** the response is streamed as it is generated by the LLM
